
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>sklearn.svm.classes &#8212; SMV Question Matching  documentation</title>
    <link rel="stylesheet" href="../../../_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
    <script type="text/javascript" src="../../../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../../_static/doctools.js"></script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
   
  <link rel="stylesheet" href="../../../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <h1>Source code for sklearn.svm.classes</h1><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">from</span> <span class="nn">.base</span> <span class="k">import</span> <span class="n">_fit_liblinear</span><span class="p">,</span> <span class="n">BaseSVC</span><span class="p">,</span> <span class="n">BaseLibSVM</span>
<span class="kn">from</span> <span class="nn">..base</span> <span class="k">import</span> <span class="n">BaseEstimator</span><span class="p">,</span> <span class="n">RegressorMixin</span>
<span class="kn">from</span> <span class="nn">..linear_model.base</span> <span class="k">import</span> <span class="n">LinearClassifierMixin</span><span class="p">,</span> <span class="n">SparseCoefMixin</span><span class="p">,</span> \
    <span class="n">LinearModel</span>
<span class="kn">from</span> <span class="nn">..utils</span> <span class="k">import</span> <span class="n">check_X_y</span>
<span class="kn">from</span> <span class="nn">..utils.validation</span> <span class="k">import</span> <span class="n">_num_samples</span>
<span class="kn">from</span> <span class="nn">..utils.multiclass</span> <span class="k">import</span> <span class="n">check_classification_targets</span>


<span class="k">class</span> <span class="nc">LinearSVC</span><span class="p">(</span><span class="n">BaseEstimator</span><span class="p">,</span> <span class="n">LinearClassifierMixin</span><span class="p">,</span>
                <span class="n">SparseCoefMixin</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Linear Support Vector Classification.</span>

<span class="sd">    Similar to SVC with parameter kernel=&#39;linear&#39;, but implemented in terms of</span>
<span class="sd">    liblinear rather than libsvm, so it has more flexibility in the choice of</span>
<span class="sd">    penalties and loss functions and should scale better to large numbers of</span>
<span class="sd">    samples.</span>

<span class="sd">    This class supports both dense and sparse input and the multiclass support</span>
<span class="sd">    is handled according to a one-vs-the-rest scheme.</span>

<span class="sd">    Read more in the :ref:`User Guide &lt;svm_classification&gt;`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    penalty : string, &#39;l1&#39; or &#39;l2&#39; (default=&#39;l2&#39;)</span>
<span class="sd">        Specifies the norm used in the penalization. The &#39;l2&#39;</span>
<span class="sd">        penalty is the standard used in SVC. The &#39;l1&#39; leads to ``coef_``</span>
<span class="sd">        vectors that are sparse.</span>

<span class="sd">    loss : string, &#39;hinge&#39; or &#39;squared_hinge&#39; (default=&#39;squared_hinge&#39;)</span>
<span class="sd">        Specifies the loss function. &#39;hinge&#39; is the standard SVM loss</span>
<span class="sd">        (used e.g. by the SVC class) while &#39;squared_hinge&#39; is the</span>
<span class="sd">        square of the hinge loss.</span>

<span class="sd">    dual : bool, (default=True)</span>
<span class="sd">        Select the algorithm to either solve the dual or primal</span>
<span class="sd">        optimization problem. Prefer dual=False when n_samples &gt; n_features.</span>

<span class="sd">    tol : float, optional (default=1e-4)</span>
<span class="sd">        Tolerance for stopping criteria.</span>

<span class="sd">    C : float, optional (default=1.0)</span>
<span class="sd">        Penalty parameter C of the error term.</span>

<span class="sd">    multi_class : string, &#39;ovr&#39; or &#39;crammer_singer&#39; (default=&#39;ovr&#39;)</span>
<span class="sd">        Determines the multi-class strategy if `y` contains more than</span>
<span class="sd">        two classes.</span>
<span class="sd">        ``&quot;ovr&quot;`` trains n_classes one-vs-rest classifiers, while</span>
<span class="sd">        ``&quot;crammer_singer&quot;`` optimizes a joint objective over all classes.</span>
<span class="sd">        While `crammer_singer` is interesting from a theoretical perspective</span>
<span class="sd">        as it is consistent, it is seldom used in practice as it rarely leads</span>
<span class="sd">        to better accuracy and is more expensive to compute.</span>
<span class="sd">        If ``&quot;crammer_singer&quot;`` is chosen, the options loss, penalty and dual</span>
<span class="sd">        will be ignored.</span>

<span class="sd">    fit_intercept : boolean, optional (default=True)</span>
<span class="sd">        Whether to calculate the intercept for this model. If set</span>
<span class="sd">        to false, no intercept will be used in calculations</span>
<span class="sd">        (i.e. data is expected to be already centered).</span>

<span class="sd">    intercept_scaling : float, optional (default=1)</span>
<span class="sd">        When self.fit_intercept is True, instance vector x becomes</span>
<span class="sd">        ``[x, self.intercept_scaling]``,</span>
<span class="sd">        i.e. a &quot;synthetic&quot; feature with constant value equals to</span>
<span class="sd">        intercept_scaling is appended to the instance vector.</span>
<span class="sd">        The intercept becomes intercept_scaling * synthetic feature weight</span>
<span class="sd">        Note! the synthetic feature weight is subject to l1/l2 regularization</span>
<span class="sd">        as all other features.</span>
<span class="sd">        To lessen the effect of regularization on synthetic feature weight</span>
<span class="sd">        (and therefore on the intercept) intercept_scaling has to be increased.</span>

<span class="sd">    class_weight : {dict, &#39;balanced&#39;}, optional</span>
<span class="sd">        Set the parameter C of class i to ``class_weight[i]*C`` for</span>
<span class="sd">        SVC. If not given, all classes are supposed to have</span>
<span class="sd">        weight one.</span>
<span class="sd">        The &quot;balanced&quot; mode uses the values of y to automatically adjust</span>
<span class="sd">        weights inversely proportional to class frequencies in the input data</span>
<span class="sd">        as ``n_samples / (n_classes * np.bincount(y))``</span>

<span class="sd">    verbose : int, (default=0)</span>
<span class="sd">        Enable verbose output. Note that this setting takes advantage of a</span>
<span class="sd">        per-process runtime setting in liblinear that, if enabled, may not work</span>
<span class="sd">        properly in a multithreaded context.</span>

<span class="sd">    random_state : int, RandomState instance or None, optional (default=None)</span>
<span class="sd">        The seed of the pseudo random number generator to use when shuffling</span>
<span class="sd">        the data.  If int, random_state is the seed used by the random number</span>
<span class="sd">        generator; If RandomState instance, random_state is the random number</span>
<span class="sd">        generator; If None, the random number generator is the RandomState</span>
<span class="sd">        instance used by `np.random`.</span>

<span class="sd">    max_iter : int, (default=1000)</span>
<span class="sd">        The maximum number of iterations to be run.</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    coef_ : array, shape = [n_features] if n_classes == 2 else [n_classes, n_features]</span>
<span class="sd">        Weights assigned to the features (coefficients in the primal</span>
<span class="sd">        problem). This is only available in the case of a linear kernel.</span>

<span class="sd">        ``coef_`` is a readonly property derived from ``raw_coef_`` that</span>
<span class="sd">        follows the internal memory layout of liblinear.</span>

<span class="sd">    intercept_ : array, shape = [1] if n_classes == 2 else [n_classes]</span>
<span class="sd">        Constants in decision function.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; from sklearn.svm import LinearSVC</span>
<span class="sd">    &gt;&gt;&gt; from sklearn.datasets import make_classification</span>
<span class="sd">    &gt;&gt;&gt; X, y = make_classification(n_features=4, random_state=0)</span>
<span class="sd">    &gt;&gt;&gt; clf = LinearSVC(random_state=0)</span>
<span class="sd">    &gt;&gt;&gt; clf.fit(X, y)</span>
<span class="sd">    LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,</span>
<span class="sd">         intercept_scaling=1, loss=&#39;squared_hinge&#39;, max_iter=1000,</span>
<span class="sd">         multi_class=&#39;ovr&#39;, penalty=&#39;l2&#39;, random_state=0, tol=0.0001,</span>
<span class="sd">         verbose=0)</span>
<span class="sd">    &gt;&gt;&gt; print(clf.coef_)</span>
<span class="sd">    [[ 0.08551385  0.39414796  0.49847831  0.37513797]]</span>
<span class="sd">    &gt;&gt;&gt; print(clf.intercept_)</span>
<span class="sd">    [ 0.28418066]</span>
<span class="sd">    &gt;&gt;&gt; print(clf.predict([[0, 0, 0, 0]]))</span>
<span class="sd">    [1]</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    The underlying C implementation uses a random number generator to</span>
<span class="sd">    select features when fitting the model. It is thus not uncommon</span>
<span class="sd">    to have slightly different results for the same input data. If</span>
<span class="sd">    that happens, try with a smaller ``tol`` parameter.</span>

<span class="sd">    The underlying implementation, liblinear, uses a sparse internal</span>
<span class="sd">    representation for the data that will incur a memory copy.</span>

<span class="sd">    Predict output may not match that of standalone liblinear in certain</span>
<span class="sd">    cases. See :ref:`differences from liblinear &lt;liblinear_differences&gt;`</span>
<span class="sd">    in the narrative documentation.</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    `LIBLINEAR: A Library for Large Linear Classification</span>
<span class="sd">    &lt;http://www.csie.ntu.edu.tw/~cjlin/liblinear/&gt;`__</span>

<span class="sd">    See also</span>
<span class="sd">    --------</span>
<span class="sd">    SVC</span>
<span class="sd">        Implementation of Support Vector Machine classifier using libsvm:</span>
<span class="sd">        the kernel can be non-linear but its SMO algorithm does not</span>
<span class="sd">        scale to large number of samples as LinearSVC does.</span>

<span class="sd">        Furthermore SVC multi-class mode is implemented using one</span>
<span class="sd">        vs one scheme while LinearSVC uses one vs the rest. It is</span>
<span class="sd">        possible to implement one vs the rest with SVC by using the</span>
<span class="sd">        :class:`sklearn.multiclass.OneVsRestClassifier` wrapper.</span>

<span class="sd">        Finally SVC can fit dense data without memory copy if the input</span>
<span class="sd">        is C-contiguous. Sparse data will still incur memory copy though.</span>

<span class="sd">    sklearn.linear_model.SGDClassifier</span>
<span class="sd">        SGDClassifier can optimize the same cost function as LinearSVC</span>
<span class="sd">        by adjusting the penalty and loss parameters. In addition it requires</span>
<span class="sd">        less memory, allows incremental (online) learning, and implements</span>
<span class="sd">        various loss functions and regularization regimes.</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">penalty</span><span class="o">=</span><span class="s1">&#39;l2&#39;</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;squared_hinge&#39;</span><span class="p">,</span> <span class="n">dual</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span>
                 <span class="n">C</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">multi_class</span><span class="o">=</span><span class="s1">&#39;ovr&#39;</span><span class="p">,</span> <span class="n">fit_intercept</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">intercept_scaling</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">class_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dual</span> <span class="o">=</span> <span class="n">dual</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tol</span> <span class="o">=</span> <span class="n">tol</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">C</span> <span class="o">=</span> <span class="n">C</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">multi_class</span> <span class="o">=</span> <span class="n">multi_class</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fit_intercept</span> <span class="o">=</span> <span class="n">fit_intercept</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">intercept_scaling</span> <span class="o">=</span> <span class="n">intercept_scaling</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">class_weight</span> <span class="o">=</span> <span class="n">class_weight</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">=</span> <span class="n">verbose</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">random_state</span> <span class="o">=</span> <span class="n">random_state</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_iter</span> <span class="o">=</span> <span class="n">max_iter</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">penalty</span> <span class="o">=</span> <span class="n">penalty</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Fit the model according to the given training data.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix}, shape = [n_samples, n_features]</span>
<span class="sd">            Training vector, where n_samples in the number of samples and</span>
<span class="sd">            n_features is the number of features.</span>

<span class="sd">        y : array-like, shape = [n_samples]</span>
<span class="sd">            Target vector relative to X</span>

<span class="sd">        sample_weight : array-like, shape = [n_samples], optional</span>
<span class="sd">            Array of weights that are assigned to individual</span>
<span class="sd">            samples. If not provided,</span>
<span class="sd">            then each sample is given unit weight.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        self : object</span>
<span class="sd">            Returns self.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># FIXME Remove l1/l2 support in 1.0 -----------------------------------</span>
        <span class="n">msg</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;loss=&#39;</span><span class="si">%s</span><span class="s2">&#39; has been deprecated in favor of &quot;</span>
               <span class="s2">&quot;loss=&#39;</span><span class="si">%s</span><span class="s2">&#39; as of 0.16. Backward compatibility&quot;</span>
               <span class="s2">&quot; for the loss=&#39;</span><span class="si">%s</span><span class="s2">&#39; will be removed in </span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="ow">in</span> <span class="p">(</span><span class="s1">&#39;l1&#39;</span><span class="p">,</span> <span class="s1">&#39;l2&#39;</span><span class="p">):</span>
            <span class="n">old_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;l1&#39;</span><span class="p">:</span> <span class="s1">&#39;hinge&#39;</span><span class="p">,</span> <span class="s1">&#39;l2&#39;</span><span class="p">:</span> <span class="s1">&#39;squared_hinge&#39;</span><span class="p">}</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">)</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="n">msg</span> <span class="o">%</span> <span class="p">(</span><span class="n">old_loss</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">,</span> <span class="n">old_loss</span><span class="p">,</span> <span class="s1">&#39;1.0&#39;</span><span class="p">),</span>
                          <span class="ne">DeprecationWarning</span><span class="p">)</span>
        <span class="c1"># ---------------------------------------------------------------------</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">C</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Penalty term must be positive; got (C=</span><span class="si">%r</span><span class="s2">)&quot;</span>
                             <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">C</span><span class="p">)</span>

        <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">check_X_y</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">accept_sparse</span><span class="o">=</span><span class="s1">&#39;csr&#39;</span><span class="p">,</span>
                         <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="s2">&quot;C&quot;</span><span class="p">)</span>
        <span class="n">check_classification_targets</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">intercept_</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_iter_</span> <span class="o">=</span> <span class="n">_fit_liblinear</span><span class="p">(</span>
            <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">C</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">fit_intercept</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">intercept_scaling</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">class_weight</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">penalty</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dual</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">max_iter</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">tol</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">multi_class</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">multi_class</span> <span class="o">==</span> <span class="s2">&quot;crammer_singer&quot;</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">fit_intercept</span><span class="p">:</span>
                <span class="n">intercept</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">intercept_</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">intercept_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">intercept_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">intercept</span><span class="p">])</span>

        <span class="k">return</span> <span class="bp">self</span>


<span class="k">class</span> <span class="nc">LinearSVR</span><span class="p">(</span><span class="n">LinearModel</span><span class="p">,</span> <span class="n">RegressorMixin</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Linear Support Vector Regression.</span>

<span class="sd">    Similar to SVR with parameter kernel=&#39;linear&#39;, but implemented in terms of</span>
<span class="sd">    liblinear rather than libsvm, so it has more flexibility in the choice of</span>
<span class="sd">    penalties and loss functions and should scale better to large numbers of</span>
<span class="sd">    samples.</span>

<span class="sd">    This class supports both dense and sparse input.</span>

<span class="sd">    Read more in the :ref:`User Guide &lt;svm_regression&gt;`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    C : float, optional (default=1.0)</span>
<span class="sd">        Penalty parameter C of the error term. The penalty is a squared</span>
<span class="sd">        l2 penalty. The bigger this parameter, the less regularization is used.</span>

<span class="sd">    loss : string, &#39;epsilon_insensitive&#39; or &#39;squared_epsilon_insensitive&#39; (default=&#39;epsilon_insensitive&#39;)</span>
<span class="sd">        Specifies the loss function. &#39;l1&#39; is the epsilon-insensitive loss</span>
<span class="sd">        (standard SVR) while &#39;l2&#39; is the squared epsilon-insensitive loss.</span>

<span class="sd">    epsilon : float, optional (default=0.1)</span>
<span class="sd">        Epsilon parameter in the epsilon-insensitive loss function. Note</span>
<span class="sd">        that the value of this parameter depends on the scale of the target</span>
<span class="sd">        variable y. If unsure, set ``epsilon=0``.</span>

<span class="sd">    dual : bool, (default=True)</span>
<span class="sd">        Select the algorithm to either solve the dual or primal</span>
<span class="sd">        optimization problem. Prefer dual=False when n_samples &gt; n_features.</span>

<span class="sd">    tol : float, optional (default=1e-4)</span>
<span class="sd">        Tolerance for stopping criteria.</span>

<span class="sd">    fit_intercept : boolean, optional (default=True)</span>
<span class="sd">        Whether to calculate the intercept for this model. If set</span>
<span class="sd">        to false, no intercept will be used in calculations</span>
<span class="sd">        (i.e. data is expected to be already centered).</span>

<span class="sd">    intercept_scaling : float, optional (default=1)</span>
<span class="sd">        When self.fit_intercept is True, instance vector x becomes</span>
<span class="sd">        [x, self.intercept_scaling],</span>
<span class="sd">        i.e. a &quot;synthetic&quot; feature with constant value equals to</span>
<span class="sd">        intercept_scaling is appended to the instance vector.</span>
<span class="sd">        The intercept becomes intercept_scaling * synthetic feature weight</span>
<span class="sd">        Note! the synthetic feature weight is subject to l1/l2 regularization</span>
<span class="sd">        as all other features.</span>
<span class="sd">        To lessen the effect of regularization on synthetic feature weight</span>
<span class="sd">        (and therefore on the intercept) intercept_scaling has to be increased.</span>

<span class="sd">    verbose : int, (default=0)</span>
<span class="sd">        Enable verbose output. Note that this setting takes advantage of a</span>
<span class="sd">        per-process runtime setting in liblinear that, if enabled, may not work</span>
<span class="sd">        properly in a multithreaded context.</span>

<span class="sd">    random_state : int, RandomState instance or None, optional (default=None)</span>
<span class="sd">        The seed of the pseudo random number generator to use when shuffling</span>
<span class="sd">        the data.  If int, random_state is the seed used by the random number</span>
<span class="sd">        generator; If RandomState instance, random_state is the random number</span>
<span class="sd">        generator; If None, the random number generator is the RandomState</span>
<span class="sd">        instance used by `np.random`.</span>

<span class="sd">    max_iter : int, (default=1000)</span>
<span class="sd">        The maximum number of iterations to be run.</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    coef_ : array, shape = [n_features] if n_classes == 2 else [n_classes, n_features]</span>
<span class="sd">        Weights assigned to the features (coefficients in the primal</span>
<span class="sd">        problem). This is only available in the case of a linear kernel.</span>

<span class="sd">        `coef_` is a readonly property derived from `raw_coef_` that</span>
<span class="sd">        follows the internal memory layout of liblinear.</span>

<span class="sd">    intercept_ : array, shape = [1] if n_classes == 2 else [n_classes]</span>
<span class="sd">        Constants in decision function.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; from sklearn.svm import LinearSVR</span>
<span class="sd">    &gt;&gt;&gt; from sklearn.datasets import make_regression</span>
<span class="sd">    &gt;&gt;&gt; X, y = make_regression(n_features=4, random_state=0)</span>
<span class="sd">    &gt;&gt;&gt; regr = LinearSVR(random_state=0)</span>
<span class="sd">    &gt;&gt;&gt; regr.fit(X, y)</span>
<span class="sd">    LinearSVR(C=1.0, dual=True, epsilon=0.0, fit_intercept=True,</span>
<span class="sd">         intercept_scaling=1.0, loss=&#39;epsilon_insensitive&#39;, max_iter=1000,</span>
<span class="sd">         random_state=0, tol=0.0001, verbose=0)</span>
<span class="sd">    &gt;&gt;&gt; print(regr.coef_)</span>
<span class="sd">    [ 16.35750999  26.91499923  42.30652207  60.47843124]</span>
<span class="sd">    &gt;&gt;&gt; print(regr.intercept_)</span>
<span class="sd">    [-4.29756543]</span>
<span class="sd">    &gt;&gt;&gt; print(regr.predict([[0, 0, 0, 0]]))</span>
<span class="sd">    [-4.29756543]</span>

<span class="sd">    See also</span>
<span class="sd">    --------</span>
<span class="sd">    LinearSVC</span>
<span class="sd">        Implementation of Support Vector Machine classifier using the</span>
<span class="sd">        same library as this class (liblinear).</span>

<span class="sd">    SVR</span>
<span class="sd">        Implementation of Support Vector Machine regression using libsvm:</span>
<span class="sd">        the kernel can be non-linear but its SMO algorithm does not</span>
<span class="sd">        scale to large number of samples as LinearSVC does.</span>

<span class="sd">    sklearn.linear_model.SGDRegressor</span>
<span class="sd">        SGDRegressor can optimize the same cost function as LinearSVR</span>
<span class="sd">        by adjusting the penalty and loss parameters. In addition it requires</span>
<span class="sd">        less memory, allows incremental (online) learning, and implements</span>
<span class="sd">        various loss functions and regularization regimes.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
                 <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;epsilon_insensitive&#39;</span><span class="p">,</span> <span class="n">fit_intercept</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">intercept_scaling</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">dual</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tol</span> <span class="o">=</span> <span class="n">tol</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">C</span> <span class="o">=</span> <span class="n">C</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="n">epsilon</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fit_intercept</span> <span class="o">=</span> <span class="n">fit_intercept</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">intercept_scaling</span> <span class="o">=</span> <span class="n">intercept_scaling</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">=</span> <span class="n">verbose</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">random_state</span> <span class="o">=</span> <span class="n">random_state</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_iter</span> <span class="o">=</span> <span class="n">max_iter</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dual</span> <span class="o">=</span> <span class="n">dual</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Fit the model according to the given training data.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix}, shape = [n_samples, n_features]</span>
<span class="sd">            Training vector, where n_samples in the number of samples and</span>
<span class="sd">            n_features is the number of features.</span>

<span class="sd">        y : array-like, shape = [n_samples]</span>
<span class="sd">            Target vector relative to X</span>

<span class="sd">        sample_weight : array-like, shape = [n_samples], optional</span>
<span class="sd">            Array of weights that are assigned to individual</span>
<span class="sd">            samples. If not provided,</span>
<span class="sd">            then each sample is given unit weight.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        self : object</span>
<span class="sd">            Returns self.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># FIXME Remove l1/l2 support in 1.0 -----------------------------------</span>
        <span class="n">msg</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;loss=&#39;</span><span class="si">%s</span><span class="s2">&#39; has been deprecated in favor of &quot;</span>
               <span class="s2">&quot;loss=&#39;</span><span class="si">%s</span><span class="s2">&#39; as of 0.16. Backward compatibility&quot;</span>
               <span class="s2">&quot; for the loss=&#39;</span><span class="si">%s</span><span class="s2">&#39; will be removed in </span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="ow">in</span> <span class="p">(</span><span class="s1">&#39;l1&#39;</span><span class="p">,</span> <span class="s1">&#39;l2&#39;</span><span class="p">):</span>
            <span class="n">old_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;l1&#39;</span><span class="p">:</span> <span class="s1">&#39;epsilon_insensitive&#39;</span><span class="p">,</span>
                         <span class="s1">&#39;l2&#39;</span><span class="p">:</span> <span class="s1">&#39;squared_epsilon_insensitive&#39;</span>
                         <span class="p">}</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">)</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="n">msg</span> <span class="o">%</span> <span class="p">(</span><span class="n">old_loss</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">,</span> <span class="n">old_loss</span><span class="p">,</span> <span class="s1">&#39;1.0&#39;</span><span class="p">),</span>
                          <span class="ne">DeprecationWarning</span><span class="p">)</span>
        <span class="c1"># ---------------------------------------------------------------------</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">C</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Penalty term must be positive; got (C=</span><span class="si">%r</span><span class="s2">)&quot;</span>
                             <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">C</span><span class="p">)</span>

        <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">check_X_y</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">accept_sparse</span><span class="o">=</span><span class="s1">&#39;csr&#39;</span><span class="p">,</span>
                         <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="s2">&quot;C&quot;</span><span class="p">)</span>
        <span class="n">penalty</span> <span class="o">=</span> <span class="s1">&#39;l2&#39;</span>  <span class="c1"># SVR only accepts l2 penalty</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">intercept_</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_iter_</span> <span class="o">=</span> <span class="n">_fit_liblinear</span><span class="p">(</span>
            <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">C</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">fit_intercept</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">intercept_scaling</span><span class="p">,</span>
            <span class="kc">None</span><span class="p">,</span> <span class="n">penalty</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dual</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">max_iter</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">tol</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">,</span>
            <span class="n">epsilon</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>

        <span class="k">return</span> <span class="bp">self</span>


<span class="k">class</span> <span class="nc">SVC</span><span class="p">(</span><span class="n">BaseSVC</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;C-Support Vector Classification.</span>

<span class="sd">    The implementation is based on libsvm. The fit time complexity</span>
<span class="sd">    is more than quadratic with the number of samples which makes it hard</span>
<span class="sd">    to scale to dataset with more than a couple of 10000 samples.</span>

<span class="sd">    The multiclass support is handled according to a one-vs-one scheme.</span>

<span class="sd">    For details on the precise mathematical formulation of the provided</span>
<span class="sd">    kernel functions and how `gamma`, `coef0` and `degree` affect each</span>
<span class="sd">    other, see the corresponding section in the narrative documentation:</span>
<span class="sd">    :ref:`svm_kernels`.</span>

<span class="sd">    Read more in the :ref:`User Guide &lt;svm_classification&gt;`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    C : float, optional (default=1.0)</span>
<span class="sd">        Penalty parameter C of the error term.</span>

<span class="sd">    kernel : string, optional (default=&#39;rbf&#39;)</span>
<span class="sd">         Specifies the kernel type to be used in the algorithm.</span>
<span class="sd">         It must be one of &#39;linear&#39;, &#39;poly&#39;, &#39;rbf&#39;, &#39;sigmoid&#39;, &#39;precomputed&#39; or</span>
<span class="sd">         a callable.</span>
<span class="sd">         If none is given, &#39;rbf&#39; will be used. If a callable is given it is</span>
<span class="sd">         used to pre-compute the kernel matrix from data matrices; that matrix</span>
<span class="sd">         should be an array of shape ``(n_samples, n_samples)``.</span>

<span class="sd">    degree : int, optional (default=3)</span>
<span class="sd">        Degree of the polynomial kernel function (&#39;poly&#39;).</span>
<span class="sd">        Ignored by all other kernels.</span>

<span class="sd">    gamma : float, optional (default=&#39;auto&#39;)</span>
<span class="sd">        Kernel coefficient for &#39;rbf&#39;, &#39;poly&#39; and &#39;sigmoid&#39;.</span>
<span class="sd">        If gamma is &#39;auto&#39; then 1/n_features will be used instead.</span>

<span class="sd">    coef0 : float, optional (default=0.0)</span>
<span class="sd">        Independent term in kernel function.</span>
<span class="sd">        It is only significant in &#39;poly&#39; and &#39;sigmoid&#39;.</span>

<span class="sd">    probability : boolean, optional (default=False)</span>
<span class="sd">        Whether to enable probability estimates. This must be enabled prior</span>
<span class="sd">        to calling `fit`, and will slow down that method.</span>

<span class="sd">    shrinking : boolean, optional (default=True)</span>
<span class="sd">        Whether to use the shrinking heuristic.</span>

<span class="sd">    tol : float, optional (default=1e-3)</span>
<span class="sd">        Tolerance for stopping criterion.</span>

<span class="sd">    cache_size : float, optional</span>
<span class="sd">        Specify the size of the kernel cache (in MB).</span>

<span class="sd">    class_weight : {dict, &#39;balanced&#39;}, optional</span>
<span class="sd">        Set the parameter C of class i to class_weight[i]*C for</span>
<span class="sd">        SVC. If not given, all classes are supposed to have</span>
<span class="sd">        weight one.</span>
<span class="sd">        The &quot;balanced&quot; mode uses the values of y to automatically adjust</span>
<span class="sd">        weights inversely proportional to class frequencies in the input data</span>
<span class="sd">        as ``n_samples / (n_classes * np.bincount(y))``</span>

<span class="sd">    verbose : bool, default: False</span>
<span class="sd">        Enable verbose output. Note that this setting takes advantage of a</span>
<span class="sd">        per-process runtime setting in libsvm that, if enabled, may not work</span>
<span class="sd">        properly in a multithreaded context.</span>

<span class="sd">    max_iter : int, optional (default=-1)</span>
<span class="sd">        Hard limit on iterations within solver, or -1 for no limit.</span>

<span class="sd">    decision_function_shape : &#39;ovo&#39;, &#39;ovr&#39;, default=&#39;ovr&#39;</span>
<span class="sd">        Whether to return a one-vs-rest (&#39;ovr&#39;) decision function of shape</span>
<span class="sd">        (n_samples, n_classes) as all other classifiers, or the original</span>
<span class="sd">        one-vs-one (&#39;ovo&#39;) decision function of libsvm which has shape</span>
<span class="sd">        (n_samples, n_classes * (n_classes - 1) / 2).</span>

<span class="sd">        .. versionchanged:: 0.19</span>
<span class="sd">            decision_function_shape is &#39;ovr&#39; by default.</span>

<span class="sd">        .. versionadded:: 0.17</span>
<span class="sd">           *decision_function_shape=&#39;ovr&#39;* is recommended.</span>

<span class="sd">        .. versionchanged:: 0.17</span>
<span class="sd">           Deprecated *decision_function_shape=&#39;ovo&#39; and None*.</span>

<span class="sd">    random_state : int, RandomState instance or None, optional (default=None)</span>
<span class="sd">        The seed of the pseudo random number generator to use when shuffling</span>
<span class="sd">        the data.  If int, random_state is the seed used by the random number</span>
<span class="sd">        generator; If RandomState instance, random_state is the random number</span>
<span class="sd">        generator; If None, the random number generator is the RandomState</span>
<span class="sd">        instance used by `np.random`.</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    support_ : array-like, shape = [n_SV]</span>
<span class="sd">        Indices of support vectors.</span>

<span class="sd">    support_vectors_ : array-like, shape = [n_SV, n_features]</span>
<span class="sd">        Support vectors.</span>

<span class="sd">    n_support_ : array-like, dtype=int32, shape = [n_class]</span>
<span class="sd">        Number of support vectors for each class.</span>

<span class="sd">    dual_coef_ : array, shape = [n_class-1, n_SV]</span>
<span class="sd">        Coefficients of the support vector in the decision function.</span>
<span class="sd">        For multiclass, coefficient for all 1-vs-1 classifiers.</span>
<span class="sd">        The layout of the coefficients in the multiclass case is somewhat</span>
<span class="sd">        non-trivial. See the section about multi-class classification in the</span>
<span class="sd">        SVM section of the User Guide for details.</span>

<span class="sd">    coef_ : array, shape = [n_class-1, n_features]</span>
<span class="sd">        Weights assigned to the features (coefficients in the primal</span>
<span class="sd">        problem). This is only available in the case of a linear kernel.</span>

<span class="sd">        `coef_` is a readonly property derived from `dual_coef_` and</span>
<span class="sd">        `support_vectors_`.</span>

<span class="sd">    intercept_ : array, shape = [n_class * (n_class-1) / 2]</span>
<span class="sd">        Constants in decision function.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import numpy as np</span>
<span class="sd">    &gt;&gt;&gt; X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])</span>
<span class="sd">    &gt;&gt;&gt; y = np.array([1, 1, 2, 2])</span>
<span class="sd">    &gt;&gt;&gt; from sklearn.svm import SVC</span>
<span class="sd">    &gt;&gt;&gt; clf = SVC()</span>
<span class="sd">    &gt;&gt;&gt; clf.fit(X, y) #doctest: +NORMALIZE_WHITESPACE</span>
<span class="sd">    SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,</span>
<span class="sd">        decision_function_shape=&#39;ovr&#39;, degree=3, gamma=&#39;auto&#39;, kernel=&#39;rbf&#39;,</span>
<span class="sd">        max_iter=-1, probability=False, random_state=None, shrinking=True,</span>
<span class="sd">        tol=0.001, verbose=False)</span>
<span class="sd">    &gt;&gt;&gt; print(clf.predict([[-0.8, -1]]))</span>
<span class="sd">    [1]</span>

<span class="sd">    See also</span>
<span class="sd">    --------</span>
<span class="sd">    SVR</span>
<span class="sd">        Support Vector Machine for Regression implemented using libsvm.</span>

<span class="sd">    LinearSVC</span>
<span class="sd">        Scalable Linear Support Vector Machine for classification</span>
<span class="sd">        implemented using liblinear. Check the See also section of</span>
<span class="sd">        LinearSVC for more comparison element.</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;rbf&#39;</span><span class="p">,</span> <span class="n">degree</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">,</span>
                 <span class="n">coef0</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">shrinking</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">probability</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">tol</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">cache_size</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">class_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">decision_function_shape</span><span class="o">=</span><span class="s1">&#39;ovr&#39;</span><span class="p">,</span>
                 <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>

        <span class="nb">super</span><span class="p">(</span><span class="n">SVC</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">impl</span><span class="o">=</span><span class="s1">&#39;c_svc&#39;</span><span class="p">,</span> <span class="n">kernel</span><span class="o">=</span><span class="n">kernel</span><span class="p">,</span> <span class="n">degree</span><span class="o">=</span><span class="n">degree</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="n">gamma</span><span class="p">,</span>
            <span class="n">coef0</span><span class="o">=</span><span class="n">coef0</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="n">tol</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="n">C</span><span class="p">,</span> <span class="n">nu</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">shrinking</span><span class="o">=</span><span class="n">shrinking</span><span class="p">,</span>
            <span class="n">probability</span><span class="o">=</span><span class="n">probability</span><span class="p">,</span> <span class="n">cache_size</span><span class="o">=</span><span class="n">cache_size</span><span class="p">,</span>
            <span class="n">class_weight</span><span class="o">=</span><span class="n">class_weight</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="n">max_iter</span><span class="p">,</span>
            <span class="n">decision_function_shape</span><span class="o">=</span><span class="n">decision_function_shape</span><span class="p">,</span>
            <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">NuSVC</span><span class="p">(</span><span class="n">BaseSVC</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Nu-Support Vector Classification.</span>

<span class="sd">    Similar to SVC but uses a parameter to control the number of support</span>
<span class="sd">    vectors.</span>

<span class="sd">    The implementation is based on libsvm.</span>

<span class="sd">    Read more in the :ref:`User Guide &lt;svm_classification&gt;`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    nu : float, optional (default=0.5)</span>
<span class="sd">        An upper bound on the fraction of training errors and a lower</span>
<span class="sd">        bound of the fraction of support vectors. Should be in the</span>
<span class="sd">        interval (0, 1].</span>

<span class="sd">    kernel : string, optional (default=&#39;rbf&#39;)</span>
<span class="sd">         Specifies the kernel type to be used in the algorithm.</span>
<span class="sd">         It must be one of &#39;linear&#39;, &#39;poly&#39;, &#39;rbf&#39;, &#39;sigmoid&#39;, &#39;precomputed&#39; or</span>
<span class="sd">         a callable.</span>
<span class="sd">         If none is given, &#39;rbf&#39; will be used. If a callable is given it is</span>
<span class="sd">         used to precompute the kernel matrix.</span>

<span class="sd">    degree : int, optional (default=3)</span>
<span class="sd">        Degree of the polynomial kernel function (&#39;poly&#39;).</span>
<span class="sd">        Ignored by all other kernels.</span>

<span class="sd">    gamma : float, optional (default=&#39;auto&#39;)</span>
<span class="sd">        Kernel coefficient for &#39;rbf&#39;, &#39;poly&#39; and &#39;sigmoid&#39;.</span>
<span class="sd">        If gamma is &#39;auto&#39; then 1/n_features will be used instead.</span>

<span class="sd">    coef0 : float, optional (default=0.0)</span>
<span class="sd">        Independent term in kernel function.</span>
<span class="sd">        It is only significant in &#39;poly&#39; and &#39;sigmoid&#39;.</span>

<span class="sd">    probability : boolean, optional (default=False)</span>
<span class="sd">        Whether to enable probability estimates. This must be enabled prior</span>
<span class="sd">        to calling `fit`, and will slow down that method.</span>

<span class="sd">    shrinking : boolean, optional (default=True)</span>
<span class="sd">        Whether to use the shrinking heuristic.</span>

<span class="sd">    tol : float, optional (default=1e-3)</span>
<span class="sd">        Tolerance for stopping criterion.</span>

<span class="sd">    cache_size : float, optional</span>
<span class="sd">        Specify the size of the kernel cache (in MB).</span>

<span class="sd">    class_weight : {dict, &#39;balanced&#39;}, optional</span>
<span class="sd">        Set the parameter C of class i to class_weight[i]*C for</span>
<span class="sd">        SVC. If not given, all classes are supposed to have</span>
<span class="sd">        weight one. The &quot;balanced&quot; mode uses the values of y to automatically</span>
<span class="sd">        adjust weights inversely proportional to class frequencies as</span>
<span class="sd">        ``n_samples / (n_classes * np.bincount(y))``</span>

<span class="sd">    verbose : bool, default: False</span>
<span class="sd">        Enable verbose output. Note that this setting takes advantage of a</span>
<span class="sd">        per-process runtime setting in libsvm that, if enabled, may not work</span>
<span class="sd">        properly in a multithreaded context.</span>

<span class="sd">    max_iter : int, optional (default=-1)</span>
<span class="sd">        Hard limit on iterations within solver, or -1 for no limit.</span>

<span class="sd">    decision_function_shape : &#39;ovo&#39;, &#39;ovr&#39;, default=&#39;ovr&#39;</span>
<span class="sd">        Whether to return a one-vs-rest (&#39;ovr&#39;) decision function of shape</span>
<span class="sd">        (n_samples, n_classes) as all other classifiers, or the original</span>
<span class="sd">        one-vs-one (&#39;ovo&#39;) decision function of libsvm which has shape</span>
<span class="sd">        (n_samples, n_classes * (n_classes - 1) / 2).</span>

<span class="sd">        .. versionchanged:: 0.19</span>
<span class="sd">            decision_function_shape is &#39;ovr&#39; by default.</span>

<span class="sd">        .. versionadded:: 0.17</span>
<span class="sd">           *decision_function_shape=&#39;ovr&#39;* is recommended.</span>

<span class="sd">        .. versionchanged:: 0.17</span>
<span class="sd">           Deprecated *decision_function_shape=&#39;ovo&#39; and None*.</span>

<span class="sd">    random_state : int, RandomState instance or None, optional (default=None)</span>
<span class="sd">        The seed of the pseudo random number generator to use when shuffling</span>
<span class="sd">        the data.  If int, random_state is the seed used by the random number</span>
<span class="sd">        generator; If RandomState instance, random_state is the random number</span>
<span class="sd">        generator; If None, the random number generator is the RandomState</span>
<span class="sd">        instance used by `np.random`.</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    support_ : array-like, shape = [n_SV]</span>
<span class="sd">        Indices of support vectors.</span>

<span class="sd">    support_vectors_ : array-like, shape = [n_SV, n_features]</span>
<span class="sd">        Support vectors.</span>

<span class="sd">    n_support_ : array-like, dtype=int32, shape = [n_class]</span>
<span class="sd">        Number of support vectors for each class.</span>

<span class="sd">    dual_coef_ : array, shape = [n_class-1, n_SV]</span>
<span class="sd">        Coefficients of the support vector in the decision function.</span>
<span class="sd">        For multiclass, coefficient for all 1-vs-1 classifiers.</span>
<span class="sd">        The layout of the coefficients in the multiclass case is somewhat</span>
<span class="sd">        non-trivial. See the section about multi-class classification in</span>
<span class="sd">        the SVM section of the User Guide for details.</span>

<span class="sd">    coef_ : array, shape = [n_class-1, n_features]</span>
<span class="sd">        Weights assigned to the features (coefficients in the primal</span>
<span class="sd">        problem). This is only available in the case of a linear kernel.</span>

<span class="sd">        `coef_` is readonly property derived from `dual_coef_` and</span>
<span class="sd">        `support_vectors_`.</span>

<span class="sd">    intercept_ : array, shape = [n_class * (n_class-1) / 2]</span>
<span class="sd">        Constants in decision function.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import numpy as np</span>
<span class="sd">    &gt;&gt;&gt; X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])</span>
<span class="sd">    &gt;&gt;&gt; y = np.array([1, 1, 2, 2])</span>
<span class="sd">    &gt;&gt;&gt; from sklearn.svm import NuSVC</span>
<span class="sd">    &gt;&gt;&gt; clf = NuSVC()</span>
<span class="sd">    &gt;&gt;&gt; clf.fit(X, y) #doctest: +NORMALIZE_WHITESPACE</span>
<span class="sd">    NuSVC(cache_size=200, class_weight=None, coef0=0.0,</span>
<span class="sd">          decision_function_shape=&#39;ovr&#39;, degree=3, gamma=&#39;auto&#39;, kernel=&#39;rbf&#39;,</span>
<span class="sd">          max_iter=-1, nu=0.5, probability=False, random_state=None,</span>
<span class="sd">          shrinking=True, tol=0.001, verbose=False)</span>
<span class="sd">    &gt;&gt;&gt; print(clf.predict([[-0.8, -1]]))</span>
<span class="sd">    [1]</span>

<span class="sd">    See also</span>
<span class="sd">    --------</span>
<span class="sd">    SVC</span>
<span class="sd">        Support Vector Machine for classification using libsvm.</span>

<span class="sd">    LinearSVC</span>
<span class="sd">        Scalable linear Support Vector Machine for classification using</span>
<span class="sd">        liblinear.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">nu</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;rbf&#39;</span><span class="p">,</span> <span class="n">degree</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">,</span> <span class="n">coef0</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
                 <span class="n">shrinking</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">probability</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">cache_size</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
                 <span class="n">class_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">decision_function_shape</span><span class="o">=</span><span class="s1">&#39;ovr&#39;</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>

        <span class="nb">super</span><span class="p">(</span><span class="n">NuSVC</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">impl</span><span class="o">=</span><span class="s1">&#39;nu_svc&#39;</span><span class="p">,</span> <span class="n">kernel</span><span class="o">=</span><span class="n">kernel</span><span class="p">,</span> <span class="n">degree</span><span class="o">=</span><span class="n">degree</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="n">gamma</span><span class="p">,</span>
            <span class="n">coef0</span><span class="o">=</span><span class="n">coef0</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="n">tol</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">nu</span><span class="o">=</span><span class="n">nu</span><span class="p">,</span> <span class="n">shrinking</span><span class="o">=</span><span class="n">shrinking</span><span class="p">,</span>
            <span class="n">probability</span><span class="o">=</span><span class="n">probability</span><span class="p">,</span> <span class="n">cache_size</span><span class="o">=</span><span class="n">cache_size</span><span class="p">,</span>
            <span class="n">class_weight</span><span class="o">=</span><span class="n">class_weight</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="n">max_iter</span><span class="p">,</span>
            <span class="n">decision_function_shape</span><span class="o">=</span><span class="n">decision_function_shape</span><span class="p">,</span>
            <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">SVR</span><span class="p">(</span><span class="n">BaseLibSVM</span><span class="p">,</span> <span class="n">RegressorMixin</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Epsilon-Support Vector Regression.</span>

<span class="sd">    The free parameters in the model are C and epsilon.</span>

<span class="sd">    The implementation is based on libsvm.</span>

<span class="sd">    Read more in the :ref:`User Guide &lt;svm_regression&gt;`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    C : float, optional (default=1.0)</span>
<span class="sd">        Penalty parameter C of the error term.</span>

<span class="sd">    epsilon : float, optional (default=0.1)</span>
<span class="sd">         Epsilon in the epsilon-SVR model. It specifies the epsilon-tube</span>
<span class="sd">         within which no penalty is associated in the training loss function</span>
<span class="sd">         with points predicted within a distance epsilon from the actual</span>
<span class="sd">         value.</span>

<span class="sd">    kernel : string, optional (default=&#39;rbf&#39;)</span>
<span class="sd">         Specifies the kernel type to be used in the algorithm.</span>
<span class="sd">         It must be one of &#39;linear&#39;, &#39;poly&#39;, &#39;rbf&#39;, &#39;sigmoid&#39;, &#39;precomputed&#39; or</span>
<span class="sd">         a callable.</span>
<span class="sd">         If none is given, &#39;rbf&#39; will be used. If a callable is given it is</span>
<span class="sd">         used to precompute the kernel matrix.</span>

<span class="sd">    degree : int, optional (default=3)</span>
<span class="sd">        Degree of the polynomial kernel function (&#39;poly&#39;).</span>
<span class="sd">        Ignored by all other kernels.</span>

<span class="sd">    gamma : float, optional (default=&#39;auto&#39;)</span>
<span class="sd">        Kernel coefficient for &#39;rbf&#39;, &#39;poly&#39; and &#39;sigmoid&#39;.</span>
<span class="sd">        If gamma is &#39;auto&#39; then 1/n_features will be used instead.</span>

<span class="sd">    coef0 : float, optional (default=0.0)</span>
<span class="sd">        Independent term in kernel function.</span>
<span class="sd">        It is only significant in &#39;poly&#39; and &#39;sigmoid&#39;.</span>

<span class="sd">    shrinking : boolean, optional (default=True)</span>
<span class="sd">        Whether to use the shrinking heuristic.</span>

<span class="sd">    tol : float, optional (default=1e-3)</span>
<span class="sd">        Tolerance for stopping criterion.</span>

<span class="sd">    cache_size : float, optional</span>
<span class="sd">        Specify the size of the kernel cache (in MB).</span>

<span class="sd">    verbose : bool, default: False</span>
<span class="sd">        Enable verbose output. Note that this setting takes advantage of a</span>
<span class="sd">        per-process runtime setting in libsvm that, if enabled, may not work</span>
<span class="sd">        properly in a multithreaded context.</span>

<span class="sd">    max_iter : int, optional (default=-1)</span>
<span class="sd">        Hard limit on iterations within solver, or -1 for no limit.</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    support_ : array-like, shape = [n_SV]</span>
<span class="sd">        Indices of support vectors.</span>

<span class="sd">    support_vectors_ : array-like, shape = [nSV, n_features]</span>
<span class="sd">        Support vectors.</span>

<span class="sd">    dual_coef_ : array, shape = [1, n_SV]</span>
<span class="sd">        Coefficients of the support vector in the decision function.</span>

<span class="sd">    coef_ : array, shape = [1, n_features]</span>
<span class="sd">        Weights assigned to the features (coefficients in the primal</span>
<span class="sd">        problem). This is only available in the case of a linear kernel.</span>

<span class="sd">        `coef_` is readonly property derived from `dual_coef_` and</span>
<span class="sd">        `support_vectors_`.</span>

<span class="sd">    intercept_ : array, shape = [1]</span>
<span class="sd">        Constants in decision function.</span>

<span class="sd">    sample_weight : array-like, shape = [n_samples]</span>
<span class="sd">            Individual weights for each sample</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; from sklearn.svm import SVR</span>
<span class="sd">    &gt;&gt;&gt; import numpy as np</span>
<span class="sd">    &gt;&gt;&gt; n_samples, n_features = 10, 5</span>
<span class="sd">    &gt;&gt;&gt; np.random.seed(0)</span>
<span class="sd">    &gt;&gt;&gt; y = np.random.randn(n_samples)</span>
<span class="sd">    &gt;&gt;&gt; X = np.random.randn(n_samples, n_features)</span>
<span class="sd">    &gt;&gt;&gt; clf = SVR(C=1.0, epsilon=0.2)</span>
<span class="sd">    &gt;&gt;&gt; clf.fit(X, y) #doctest: +NORMALIZE_WHITESPACE</span>
<span class="sd">    SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.2, gamma=&#39;auto&#39;,</span>
<span class="sd">        kernel=&#39;rbf&#39;, max_iter=-1, shrinking=True, tol=0.001, verbose=False)</span>

<span class="sd">    See also</span>
<span class="sd">    --------</span>
<span class="sd">    NuSVR</span>
<span class="sd">        Support Vector Machine for regression implemented using libsvm</span>
<span class="sd">        using a parameter to control the number of support vectors.</span>

<span class="sd">    LinearSVR</span>
<span class="sd">        Scalable Linear Support Vector Machine for regression</span>
<span class="sd">        implemented using liblinear.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;rbf&#39;</span><span class="p">,</span> <span class="n">degree</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">,</span> <span class="n">coef0</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
                 <span class="n">tol</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">shrinking</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">cache_size</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>

        <span class="nb">super</span><span class="p">(</span><span class="n">SVR</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="s1">&#39;epsilon_svr&#39;</span><span class="p">,</span> <span class="n">kernel</span><span class="o">=</span><span class="n">kernel</span><span class="p">,</span> <span class="n">degree</span><span class="o">=</span><span class="n">degree</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="n">gamma</span><span class="p">,</span>
            <span class="n">coef0</span><span class="o">=</span><span class="n">coef0</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="n">tol</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="n">C</span><span class="p">,</span> <span class="n">nu</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="n">epsilon</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">,</span>
            <span class="n">shrinking</span><span class="o">=</span><span class="n">shrinking</span><span class="p">,</span> <span class="n">probability</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">cache_size</span><span class="o">=</span><span class="n">cache_size</span><span class="p">,</span>
            <span class="n">class_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="n">max_iter</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">NuSVR</span><span class="p">(</span><span class="n">BaseLibSVM</span><span class="p">,</span> <span class="n">RegressorMixin</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Nu Support Vector Regression.</span>

<span class="sd">    Similar to NuSVC, for regression, uses a parameter nu to control</span>
<span class="sd">    the number of support vectors. However, unlike NuSVC, where nu</span>
<span class="sd">    replaces C, here nu replaces the parameter epsilon of epsilon-SVR.</span>

<span class="sd">    The implementation is based on libsvm.</span>

<span class="sd">    Read more in the :ref:`User Guide &lt;svm_regression&gt;`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    C : float, optional (default=1.0)</span>
<span class="sd">        Penalty parameter C of the error term.</span>

<span class="sd">    nu : float, optional</span>
<span class="sd">        An upper bound on the fraction of training errors and a lower bound of</span>
<span class="sd">        the fraction of support vectors. Should be in the interval (0, 1].  By</span>
<span class="sd">        default 0.5 will be taken.</span>

<span class="sd">    kernel : string, optional (default=&#39;rbf&#39;)</span>
<span class="sd">         Specifies the kernel type to be used in the algorithm.</span>
<span class="sd">         It must be one of &#39;linear&#39;, &#39;poly&#39;, &#39;rbf&#39;, &#39;sigmoid&#39;, &#39;precomputed&#39; or</span>
<span class="sd">         a callable.</span>
<span class="sd">         If none is given, &#39;rbf&#39; will be used. If a callable is given it is</span>
<span class="sd">         used to precompute the kernel matrix.</span>

<span class="sd">    degree : int, optional (default=3)</span>
<span class="sd">        Degree of the polynomial kernel function (&#39;poly&#39;).</span>
<span class="sd">        Ignored by all other kernels.</span>

<span class="sd">    gamma : float, optional (default=&#39;auto&#39;)</span>
<span class="sd">        Kernel coefficient for &#39;rbf&#39;, &#39;poly&#39; and &#39;sigmoid&#39;.</span>
<span class="sd">        If gamma is &#39;auto&#39; then 1/n_features will be used instead.</span>

<span class="sd">    coef0 : float, optional (default=0.0)</span>
<span class="sd">        Independent term in kernel function.</span>
<span class="sd">        It is only significant in &#39;poly&#39; and &#39;sigmoid&#39;.</span>

<span class="sd">    shrinking : boolean, optional (default=True)</span>
<span class="sd">        Whether to use the shrinking heuristic.</span>

<span class="sd">    tol : float, optional (default=1e-3)</span>
<span class="sd">        Tolerance for stopping criterion.</span>

<span class="sd">    cache_size : float, optional</span>
<span class="sd">        Specify the size of the kernel cache (in MB).</span>

<span class="sd">    verbose : bool, default: False</span>
<span class="sd">        Enable verbose output. Note that this setting takes advantage of a</span>
<span class="sd">        per-process runtime setting in libsvm that, if enabled, may not work</span>
<span class="sd">        properly in a multithreaded context.</span>

<span class="sd">    max_iter : int, optional (default=-1)</span>
<span class="sd">        Hard limit on iterations within solver, or -1 for no limit.</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    support_ : array-like, shape = [n_SV]</span>
<span class="sd">        Indices of support vectors.</span>

<span class="sd">    support_vectors_ : array-like, shape = [nSV, n_features]</span>
<span class="sd">        Support vectors.</span>

<span class="sd">    dual_coef_ : array, shape = [1, n_SV]</span>
<span class="sd">        Coefficients of the support vector in the decision function.</span>

<span class="sd">    coef_ : array, shape = [1, n_features]</span>
<span class="sd">        Weights assigned to the features (coefficients in the primal</span>
<span class="sd">        problem). This is only available in the case of a linear kernel.</span>

<span class="sd">        `coef_` is readonly property derived from `dual_coef_` and</span>
<span class="sd">        `support_vectors_`.</span>

<span class="sd">    intercept_ : array, shape = [1]</span>
<span class="sd">        Constants in decision function.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; from sklearn.svm import NuSVR</span>
<span class="sd">    &gt;&gt;&gt; import numpy as np</span>
<span class="sd">    &gt;&gt;&gt; n_samples, n_features = 10, 5</span>
<span class="sd">    &gt;&gt;&gt; np.random.seed(0)</span>
<span class="sd">    &gt;&gt;&gt; y = np.random.randn(n_samples)</span>
<span class="sd">    &gt;&gt;&gt; X = np.random.randn(n_samples, n_features)</span>
<span class="sd">    &gt;&gt;&gt; clf = NuSVR(C=1.0, nu=0.1)</span>
<span class="sd">    &gt;&gt;&gt; clf.fit(X, y)  #doctest: +NORMALIZE_WHITESPACE</span>
<span class="sd">    NuSVR(C=1.0, cache_size=200, coef0=0.0, degree=3, gamma=&#39;auto&#39;,</span>
<span class="sd">          kernel=&#39;rbf&#39;, max_iter=-1, nu=0.1, shrinking=True, tol=0.001,</span>
<span class="sd">          verbose=False)</span>

<span class="sd">    See also</span>
<span class="sd">    --------</span>
<span class="sd">    NuSVC</span>
<span class="sd">        Support Vector Machine for classification implemented with libsvm</span>
<span class="sd">        with a parameter to control the number of support vectors.</span>

<span class="sd">    SVR</span>
<span class="sd">        epsilon Support Vector Machine for regression implemented with libsvm.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">nu</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;rbf&#39;</span><span class="p">,</span> <span class="n">degree</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                 <span class="n">gamma</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">,</span> <span class="n">coef0</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">shrinking</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span>
                 <span class="n">cache_size</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>

        <span class="nb">super</span><span class="p">(</span><span class="n">NuSVR</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="s1">&#39;nu_svr&#39;</span><span class="p">,</span> <span class="n">kernel</span><span class="o">=</span><span class="n">kernel</span><span class="p">,</span> <span class="n">degree</span><span class="o">=</span><span class="n">degree</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="n">gamma</span><span class="p">,</span> <span class="n">coef0</span><span class="o">=</span><span class="n">coef0</span><span class="p">,</span>
            <span class="n">tol</span><span class="o">=</span><span class="n">tol</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="n">C</span><span class="p">,</span> <span class="n">nu</span><span class="o">=</span><span class="n">nu</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">shrinking</span><span class="o">=</span><span class="n">shrinking</span><span class="p">,</span>
            <span class="n">probability</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">cache_size</span><span class="o">=</span><span class="n">cache_size</span><span class="p">,</span> <span class="n">class_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="n">max_iter</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">OneClassSVM</span><span class="p">(</span><span class="n">BaseLibSVM</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Unsupervised Outlier Detection.</span>

<span class="sd">    Estimate the support of a high-dimensional distribution.</span>

<span class="sd">    The implementation is based on libsvm.</span>

<span class="sd">    Read more in the :ref:`User Guide &lt;svm_outlier_detection&gt;`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    kernel : string, optional (default=&#39;rbf&#39;)</span>
<span class="sd">         Specifies the kernel type to be used in the algorithm.</span>
<span class="sd">         It must be one of &#39;linear&#39;, &#39;poly&#39;, &#39;rbf&#39;, &#39;sigmoid&#39;, &#39;precomputed&#39; or</span>
<span class="sd">         a callable.</span>
<span class="sd">         If none is given, &#39;rbf&#39; will be used. If a callable is given it is</span>
<span class="sd">         used to precompute the kernel matrix.</span>

<span class="sd">    nu : float, optional</span>
<span class="sd">        An upper bound on the fraction of training</span>
<span class="sd">        errors and a lower bound of the fraction of support</span>
<span class="sd">        vectors. Should be in the interval (0, 1]. By default 0.5</span>
<span class="sd">        will be taken.</span>

<span class="sd">    degree : int, optional (default=3)</span>
<span class="sd">        Degree of the polynomial kernel function (&#39;poly&#39;).</span>
<span class="sd">        Ignored by all other kernels.</span>

<span class="sd">    gamma : float, optional (default=&#39;auto&#39;)</span>
<span class="sd">        Kernel coefficient for &#39;rbf&#39;, &#39;poly&#39; and &#39;sigmoid&#39;.</span>
<span class="sd">        If gamma is &#39;auto&#39; then 1/n_features will be used instead.</span>

<span class="sd">    coef0 : float, optional (default=0.0)</span>
<span class="sd">        Independent term in kernel function.</span>
<span class="sd">        It is only significant in &#39;poly&#39; and &#39;sigmoid&#39;.</span>

<span class="sd">    tol : float, optional</span>
<span class="sd">        Tolerance for stopping criterion.</span>

<span class="sd">    shrinking : boolean, optional</span>
<span class="sd">        Whether to use the shrinking heuristic.</span>

<span class="sd">    cache_size : float, optional</span>
<span class="sd">        Specify the size of the kernel cache (in MB).</span>

<span class="sd">    verbose : bool, default: False</span>
<span class="sd">        Enable verbose output. Note that this setting takes advantage of a</span>
<span class="sd">        per-process runtime setting in libsvm that, if enabled, may not work</span>
<span class="sd">        properly in a multithreaded context.</span>

<span class="sd">    max_iter : int, optional (default=-1)</span>
<span class="sd">        Hard limit on iterations within solver, or -1 for no limit.</span>

<span class="sd">    random_state : int, RandomState instance or None, optional (default=None)</span>
<span class="sd">        The seed of the pseudo random number generator to use when shuffling</span>
<span class="sd">        the data.  If int, random_state is the seed used by the random number</span>
<span class="sd">        generator; If RandomState instance, random_state is the random number</span>
<span class="sd">        generator; If None, the random number generator is the RandomState</span>
<span class="sd">        instance used by `np.random`.</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    support_ : array-like, shape = [n_SV]</span>
<span class="sd">        Indices of support vectors.</span>

<span class="sd">    support_vectors_ : array-like, shape = [nSV, n_features]</span>
<span class="sd">        Support vectors.</span>

<span class="sd">    dual_coef_ : array, shape = [1, n_SV]</span>
<span class="sd">        Coefficients of the support vectors in the decision function.</span>

<span class="sd">    coef_ : array, shape = [1, n_features]</span>
<span class="sd">        Weights assigned to the features (coefficients in the primal</span>
<span class="sd">        problem). This is only available in the case of a linear kernel.</span>

<span class="sd">        `coef_` is readonly property derived from `dual_coef_` and</span>
<span class="sd">        `support_vectors_`</span>

<span class="sd">    intercept_ : array, shape = [1,]</span>
<span class="sd">        Constant in the decision function.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;rbf&#39;</span><span class="p">,</span> <span class="n">degree</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">,</span> <span class="n">coef0</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
                 <span class="n">tol</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">nu</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">shrinking</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">cache_size</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
                 <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>

        <span class="nb">super</span><span class="p">(</span><span class="n">OneClassSVM</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="s1">&#39;one_class&#39;</span><span class="p">,</span> <span class="n">kernel</span><span class="p">,</span> <span class="n">degree</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">coef0</span><span class="p">,</span> <span class="n">tol</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="n">nu</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span>
            <span class="n">shrinking</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="n">cache_size</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">verbose</span><span class="p">,</span> <span class="n">max_iter</span><span class="p">,</span>
            <span class="n">random_state</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">params</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Detects the soft boundary of the set of samples X.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix}, shape (n_samples, n_features)</span>
<span class="sd">            Set of samples, where n_samples is the number of samples and</span>
<span class="sd">            n_features is the number of features.</span>

<span class="sd">        sample_weight : array-like, shape (n_samples,)</span>
<span class="sd">            Per-sample weights. Rescale C per sample. Higher weights</span>
<span class="sd">            force the classifier to put more emphasis on these points.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        self : object</span>
<span class="sd">            Returns self.</span>

<span class="sd">        Notes</span>
<span class="sd">        -----</span>
<span class="sd">        If X is not a C-ordered contiguous array it is copied.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">OneClassSVM</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">_num_samples</span><span class="p">(</span><span class="n">X</span><span class="p">)),</span>
                                     <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">,</span> <span class="o">**</span><span class="n">params</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="nf">decision_function</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Signed distance to the separating hyperplane.</span>

<span class="sd">        Signed distance is positive for an inlier and negative for an outlier.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : array-like, shape (n_samples, n_features)</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        X : array-like, shape (n_samples,)</span>
<span class="sd">            Returns the decision function of the samples.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">dec</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_decision_function</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">dec</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Perform classification on samples in X.</span>

<span class="sd">        For an one-class model, +1 or -1 is returned.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix}, shape (n_samples, n_features)</span>
<span class="sd">            For kernel=&quot;precomputed&quot;, the expected shape of X is</span>
<span class="sd">            [n_samples_test, n_samples_train]</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        y_pred : array, shape (n_samples,)</span>
<span class="sd">            Class labels for samples in X.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">y</span> <span class="o">=</span> <span class="nb">super</span><span class="p">(</span><span class="n">OneClassSVM</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">intp</span><span class="p">)</span>
</pre></div>

          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper"><div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../../../index.html">Documentation overview</a><ul>
  <li><a href="../../index.html">Module code</a><ul>
  </ul></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../../search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    </div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2018, Zach Brownlow, Christina Mara, Will Rea, Matt Rosenbloom.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 1.7.2</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.10</a>
      
    </div>

    

    
  </body>
</html>